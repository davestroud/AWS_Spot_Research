{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cbbe2db265ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m## For Prophet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "## For data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "## For Arima\n",
    "import pmdarima\n",
    "import statsmodels.tsa.api as smt\n",
    "## For Lstm\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "## For Prophet\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = pd.read_csv('sales_train.csv')\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## format datetime column\n",
    "dtf[\"date\"] = pd.to_datetime(dtf['date'], format='%d.%m.%Y')\n",
    "## create time series\n",
    "ts = dtf.groupby(\"date\")[\"item_cnt_day\"].sum().rename(\"sales\")\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split train/test from any given data point.\n",
    ":parameter\n",
    "    :param ts: pandas Series\n",
    "    :param test: num or str - test size (ex. 0.20) or index position\n",
    "                 (ex. \"yyyy-mm-dd\", 1000)\n",
    ":return\n",
    "    ts_train, ts_test\n",
    "'''\n",
    "def split_train_test(ts, test=0.20, plot=True, figsize=(15,5)):\n",
    "    ## define splitting point\n",
    "    if type(test) is float:\n",
    "        split = int(len(ts)*(1-test))\n",
    "        perc = test\n",
    "    elif type(test) is str:\n",
    "        split = ts.reset_index()[ \n",
    "                      ts.reset_index().iloc[:,0]==test].index[0]\n",
    "        perc = round(len(ts[split:])/len(ts), 2)\n",
    "    else:\n",
    "        split = test\n",
    "        perc = round(len(ts[split:])/len(ts), 2)\n",
    "    print(\"--- splitting at index: \", split, \"|\", \n",
    "          ts.index[split], \"| test size:\", perc, \" ---\")\n",
    "    \n",
    "    ## split ts\n",
    "    ts_train = ts.head(split)\n",
    "    ts_test = ts.tail(len(ts)-split)\n",
    "    if plot is True:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, \n",
    "                               sharey=True, figsize=figsize)\n",
    "        ts_train.plot(ax=ax[0], grid=True, title=\"Train\", \n",
    "                      color=\"black\")\n",
    "        ts_test.plot(ax=ax[1], grid=True, title=\"Test\", \n",
    "                     color=\"black\")\n",
    "        ax[0].set(xlabel=None)\n",
    "        ax[1].set(xlabel=None)\n",
    "        plt.show()\n",
    "        \n",
    "    return ts_train, ts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train, ts_test = split_train_test(ts, test=\"2015-06-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluation metrics for predictions.\n",
    ":parameter\n",
    "    :param dtf: DataFrame with columns raw values, fitted training  \n",
    "                 values, predicted test values\n",
    ":return\n",
    "    dataframe with raw ts and forecast\n",
    "'''\n",
    "def utils_evaluate_forecast(dtf, title, plot=True, figsize=(20,13)):\n",
    "    try:\n",
    "        ## residuals\n",
    "        dtf[\"residuals\"] = dtf[\"ts\"] - dtf[\"model\"]\n",
    "        dtf[\"error\"] = dtf[\"ts\"] - dtf[\"forecast\"]\n",
    "        dtf[\"error_pct\"] = dtf[\"error\"] / dtf[\"ts\"]\n",
    "        \n",
    "        ## kpi\n",
    "        residuals_mean = dtf[\"residuals\"].mean()\n",
    "        residuals_std = dtf[\"residuals\"].std()\n",
    "        error_mean = dtf[\"error\"].mean()\n",
    "        error_std = dtf[\"error\"].std()\n",
    "        mae = dtf[\"error\"].apply(lambda x: np.abs(x)).mean()\n",
    "        mape = dtf[\"error_pct\"].apply(lambda x: np.abs(x)).mean()  \n",
    "        mse = dtf[\"error\"].apply(lambda x: x**2).mean()\n",
    "        rmse = np.sqrt(mse)  #root mean squared error\n",
    "        \n",
    "        ## intervals\n",
    "        dtf[\"conf_int_low\"] = dtf[\"forecast\"] - 1.96*residuals_std\n",
    "        dtf[\"conf_int_up\"] = dtf[\"forecast\"] + 1.96*residuals_std\n",
    "        dtf[\"pred_int_low\"] = dtf[\"forecast\"] - 1.96*error_std\n",
    "        dtf[\"pred_int_up\"] = dtf[\"forecast\"] + 1.96*error_std\n",
    "        \n",
    "        ## plot\n",
    "        if plot==True:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            fig.suptitle(title, fontsize=20)   \n",
    "            ax1 = fig.add_subplot(2,2, 1)\n",
    "            ax2 = fig.add_subplot(2,2, 2, sharey=ax1)\n",
    "            ax3 = fig.add_subplot(2,2, 3)\n",
    "            ax4 = fig.add_subplot(2,2, 4)\n",
    "            ### training\n",
    "            dtf[pd.notnull(dtf[\"model\"])][[\"ts\",\"model\"]].plot(color=[\"black\",\"green\"], title=\"Model\", grid=True, ax=ax1)      \n",
    "            ax1.set(xlabel=None)\n",
    "            ### test\n",
    "            dtf[pd.isnull(dtf[\"model\"])][[\"ts\",\"forecast\"]].plot(color=[\"black\",\"red\"], title=\"Forecast\", grid=True, ax=ax2)\n",
    "            ax2.fill_between(x=dtf.index, y1=dtf['pred_int_low'], y2=dtf['pred_int_up'], color='b', alpha=0.2)\n",
    "            ax2.fill_between(x=dtf.index, y1=dtf['conf_int_low'], y2=dtf['conf_int_up'], color='b', alpha=0.3)     \n",
    "            ax2.set(xlabel=None)\n",
    "            ### residuals\n",
    "            dtf[[\"residuals\",\"error\"]].plot(ax=ax3, color=[\"green\",\"red\"], title=\"Residuals\", grid=True)\n",
    "            ax3.set(xlabel=None)\n",
    "            ### residuals distribution\n",
    "            dtf[[\"residuals\",\"error\"]].plot(ax=ax4, color=[\"green\",\"red\"], kind='kde', title=\"Residuals Distribution\", grid=True)\n",
    "            ax4.set(ylabel=None)\n",
    "            plt.show()\n",
    "            print(\"Training --> Residuals mean:\", np.round(residuals_mean), \" | std:\", np.round(residuals_std))\n",
    "            print(\"Test --> Error mean:\", np.round(error_mean), \" | std:\", np.round(error_std),\n",
    "                  \" | mae:\",np.round(mae), \" | mape:\",np.round(mape*100), \"%  | mse:\",np.round(mse), \" | rmse:\",np.round(rmse))\n",
    "        \n",
    "        return dtf[[\"ts\",\"model\",\"residuals\",\"conf_int_low\",\"conf_int_up\", \n",
    "                    \"forecast\",\"error\",\"pred_int_low\",\"pred_int_up\"]]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"--- got error ---\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pmdarima.auto_arima(ts, exogenous=exog,                                    \n",
    "                                 seasonal=True, stationary=False, \n",
    "                                 m=7, information_criterion='aic', \n",
    "                                 max_order=20,                                     \n",
    "                                 max_p=10, max_d=3, max_q=10,                                     \n",
    "                                 max_P=10, max_D=3, max_Q=10,                                   \n",
    "                                 error_action='ignore')\n",
    "print(\"best model --> (p, d, q):\", best_model.order, \" and  (P, D, Q, s):\", best_model.seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
